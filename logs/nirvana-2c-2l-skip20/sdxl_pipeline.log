[SUYI] Using EulerDiscreteScheduler
[SUYI] In unet_2d_condition.py
[SUYI] USE_PEFT_BACKEND=True in loaders/lora.py
Args Namespace(skipped_steps=20, enable_hit_ratio=False, serve_mode='standard', lora_mode='full', load_lora_mode='default', num_controlnets=2, num_loras=2)
Nirvana config {'K': 20, 'hit_ratio': 0.2}
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] load_state_dict: safetensors.torch.load_file(/home/slida/.cache/huggingface/hub/models--diffusers--controlnet-depth-sdxl-1.0/snapshots/17bb97973f29801224cd66f192c5ffacf82648b4/diffusion_pytorch_model.fp16.safetensors)
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] load_state_dict: safetensors.torch.load_file(/home/slida/.cache/huggingface/hub/models--diffusers--controlnet-depth-sdxl-1.0/snapshots/17bb97973f29801224cd66f192c5ffacf82648b4/diffusion_pytorch_model.fp16.safetensors)
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] load_state_dict: safetensors.torch.load_file(/home/slida/.cache/huggingface/hub/models--madebyollin--sdxl-vae-fp16-fix/snapshots/207b116dae70ace3637169f1ddd2434b91b3a8cd/diffusion_pytorch_model.safetensors)
[SUYI] Using customized pipeline_stable_diffusion_xl
Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]Loading pipeline components...:  29%|██▊       | 2/7 [00:00<00:00, 13.43it/s]Loading pipeline components...:  57%|█████▋    | 4/7 [00:13<00:11,  3.83s/it]Loading pipeline components...:  71%|███████▏  | 5/7 [00:13<00:05,  2.79s/it]Loading pipeline components...: 100%|██████████| 7/7 [00:13<00:00,  1.62s/it]Loading pipeline components...: 100%|██████████| 7/7 [00:13<00:00,  1.98s/it]
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_geglu: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_channels_last: False
[LINGYUN] norm_num_groups: 32
[LINGYUN] enable_fused_norm_silu:  False
[LINGYUN] enable_cuda_graph: False
[LINGYUN] load_state_dict: safetensors.torch.load_file(/home/slida/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet/diffusion_pytorch_model.fp16.safetensors)
0 by william eggleston, a long wooden bench in front of a brick wall, 4k, clean background
[SUYI] load_file_time=0.480
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.986
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.13
[SUYI] load_lora_into_unet_time=2.147
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.128
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.494
[SUYI] load_file_time=0.621
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.509
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.69
[SUYI] load_lora_into_unet_time=2.700
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:07,  4.04it/s] 10%|█         | 3/30 [00:00<00:03,  7.34it/s] 13%|█▎        | 4/30 [00:00<00:03,  7.87it/s] 17%|█▋        | 5/30 [00:00<00:03,  8.24it/s] 20%|██        | 6/30 [00:00<00:02,  8.51it/s] 23%|██▎       | 7/30 [00:00<00:02,  8.70it/s] 27%|██▋       | 8/30 [00:00<00:02,  8.84it/s] 30%|███       | 9/30 [00:01<00:02,  8.93it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.00it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.05it/s] 40%|████      | 12/30 [00:01<00:01,  9.08it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.11it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.12it/s] 50%|█████     | 15/30 [00:01<00:01,  9.13it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.13it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.14it/s] 60%|██████    | 18/30 [00:02<00:01,  9.14it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.15it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.15it/s] 70%|███████   | 21/30 [00:02<00:00,  9.15it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.15it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.15it/s] 80%|████████  | 24/30 [00:02<00:00,  9.15it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.15it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.15it/s] 90%|█████████ | 27/30 [00:03<00:00,  9.15it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.12it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.17it/s]100%|██████████| 30/30 [00:03<00:00,  9.16it/s]100%|██████████| 30/30 [00:03<00:00,  8.84it/s]
Load LoRA latency: 7.23
End2End inference latency: 11.25
==============================
1 by william eggleston, a cat looking out of a window, 4k, clean background
[SUYI] load_file_time=0.484
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.762
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 1.92
[SUYI] load_lora_into_unet_time=1.929
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.108
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.428
[SUYI] load_file_time=0.488
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.345
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.54
[SUYI] load_lora_into_unet_time=2.547
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.26it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.56it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.43it/s] 20%|██        | 6/30 [00:00<00:02,  9.35it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.29it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.24it/s] 30%|███       | 9/30 [00:00<00:02,  9.21it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.19it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.17it/s] 40%|████      | 12/30 [00:01<00:01,  9.16it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.15it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.15it/s] 50%|█████     | 15/30 [00:01<00:01,  9.15it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.15it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.14it/s] 60%|██████    | 18/30 [00:01<00:01,  9.14it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.14it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.14it/s] 70%|███████   | 21/30 [00:02<00:00,  9.14it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.14it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.14it/s] 80%|████████  | 24/30 [00:02<00:00,  9.14it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.14it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.14it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.14it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.14it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.14it/s]100%|██████████| 30/30 [00:03<00:00,  9.14it/s]100%|██████████| 30/30 [00:03<00:00,  9.21it/s]
Load LoRA latency: 6.26
End2End inference latency: 9.77
==============================
2 by william eggleston, A helicopter flies over Yosemite., 4k, clean background
[SUYI] load_file_time=0.521
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.696
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 1.85
[SUYI] load_lora_into_unet_time=1.863
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.215
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.430
[SUYI] load_file_time=0.486
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.207
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.40
[SUYI] load_lora_into_unet_time=2.411
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.26it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.58it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.45it/s] 20%|██        | 6/30 [00:00<00:02,  9.36it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.30it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.25it/s] 30%|███       | 9/30 [00:00<00:02,  9.22it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.20it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.19it/s] 40%|████      | 12/30 [00:01<00:01,  9.18it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.17it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.17it/s] 50%|█████     | 15/30 [00:01<00:01,  9.16it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.16it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.15it/s] 60%|██████    | 18/30 [00:01<00:01,  9.16it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.15it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.15it/s] 70%|███████   | 21/30 [00:02<00:00,  9.15it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.15it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.15it/s] 80%|████████  | 24/30 [00:02<00:00,  9.16it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.15it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.15it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.15it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.15it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.16it/s]100%|██████████| 30/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.22it/s]
Load LoRA latency: 6.21
End2End inference latency: 9.72
==============================
3 by william eggleston, a handsaw on a table, 4k, clean background
[SUYI] load_file_time=0.498
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.051
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.21
[SUYI] load_lora_into_unet_time=2.225
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.121
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.636
[SUYI] load_file_time=0.467
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.458
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.65
[SUYI] load_lora_into_unet_time=2.662
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.27it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.57it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.44it/s] 20%|██        | 6/30 [00:00<00:02,  9.35it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.28it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.24it/s] 30%|███       | 9/30 [00:00<00:02,  9.21it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.19it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.17it/s] 40%|████      | 12/30 [00:01<00:01,  9.16it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.15it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.08it/s] 50%|█████     | 15/30 [00:01<00:01,  9.17it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.16it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.15it/s] 60%|██████    | 18/30 [00:01<00:01,  9.15it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.14it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.14it/s] 70%|███████   | 21/30 [00:02<00:00,  9.14it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.14it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.14it/s] 80%|████████  | 24/30 [00:02<00:00,  9.14it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.14it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.14it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.14it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.14it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.14it/s]100%|██████████| 30/30 [00:03<00:00,  9.14it/s]100%|██████████| 30/30 [00:03<00:00,  9.20it/s]
Load LoRA latency: 6.90
End2End inference latency: 10.42
==============================
4 by william eggleston, A smiling sloth wearing a bowtie and holding a quarterstaff and a big book. A shiny VW van parked on grass., 4k, clean background
[SUYI] load_file_time=0.488
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.936
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.09
[SUYI] load_lora_into_unet_time=2.105
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.119
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.483
[SUYI] load_file_time=0.474
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.534
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.73
[SUYI] load_lora_into_unet_time=2.737
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.28it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.59it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.45it/s] 20%|██        | 6/30 [00:00<00:02,  9.36it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.30it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.25it/s] 30%|███       | 9/30 [00:00<00:02,  9.22it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.20it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.19it/s] 40%|████      | 12/30 [00:01<00:01,  9.18it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.17it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.16it/s] 50%|█████     | 15/30 [00:01<00:01,  9.16it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.16it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.16it/s] 60%|██████    | 18/30 [00:01<00:01,  9.16it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.16it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.16it/s] 70%|███████   | 21/30 [00:02<00:00,  9.15it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.12it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.16it/s] 80%|████████  | 24/30 [00:02<00:00,  9.16it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.16it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.15it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.15it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.15it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.22it/s]
Load LoRA latency: 6.68
End2End inference latency: 10.20
==============================
5 by william eggleston, a large present with a red ribbon, 4k, clean background
[SUYI] load_file_time=0.485
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.952
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.11
[SUYI] load_lora_into_unet_time=2.119
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.122
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.469
[SUYI] load_file_time=0.485
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.417
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.61
[SUYI] load_lora_into_unet_time=2.622
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.26it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.57it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.44it/s] 20%|██        | 6/30 [00:00<00:02,  9.35it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.28it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.24it/s] 30%|███       | 9/30 [00:00<00:02,  9.21it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.19it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.17it/s] 40%|████      | 12/30 [00:01<00:01,  9.16it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.15it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.15it/s] 50%|█████     | 15/30 [00:01<00:01,  9.15it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.14it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.14it/s] 60%|██████    | 18/30 [00:01<00:01,  9.14it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.14it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.14it/s] 70%|███████   | 21/30 [00:02<00:00,  9.14it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.14it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.14it/s] 80%|████████  | 24/30 [00:02<00:00,  9.14it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.14it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.13it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.13it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.14it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.13it/s]100%|██████████| 30/30 [00:03<00:00,  9.14it/s]100%|██████████| 30/30 [00:03<00:00,  9.20it/s]
Load LoRA latency: 6.58
End2End inference latency: 10.10
==============================
6 by william eggleston, a cat reading a book, 4k, clean background
[SUYI] load_file_time=0.488
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.948
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.10
[SUYI] load_lora_into_unet_time=2.116
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.122
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.590
[SUYI] load_file_time=0.469
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.444
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.64
[SUYI] load_lora_into_unet_time=2.647
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.27it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.58it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.45it/s] 20%|██        | 6/30 [00:00<00:02,  9.36it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.30it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.25it/s] 30%|███       | 9/30 [00:00<00:02,  9.22it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.20it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.19it/s] 40%|████      | 12/30 [00:01<00:01,  9.18it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.17it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.16it/s] 50%|█████     | 15/30 [00:01<00:01,  9.16it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.16it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.16it/s] 60%|██████    | 18/30 [00:01<00:01,  9.15it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.15it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.15it/s] 70%|███████   | 21/30 [00:02<00:00,  9.15it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.14it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.15it/s] 80%|████████  | 24/30 [00:02<00:00,  9.15it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.15it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.15it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.15it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.15it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.22it/s]
Load LoRA latency: 6.71
End2End inference latency: 10.22
==============================
7 by william eggleston, the Beatles crossing Abbey road, 4k, clean background
[SUYI] load_file_time=0.485
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.948
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.10
[SUYI] load_lora_into_unet_time=2.117
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.122
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.590
[SUYI] load_file_time=0.455
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.436
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.63
[SUYI] load_lora_into_unet_time=2.638
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.27it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.57it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.44it/s] 20%|██        | 6/30 [00:00<00:02,  9.35it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.28it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.24it/s] 30%|███       | 9/30 [00:00<00:02,  9.21it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.19it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.17it/s] 40%|████      | 12/30 [00:01<00:01,  9.16it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.15it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.14it/s] 50%|█████     | 15/30 [00:01<00:01,  9.14it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.14it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.14it/s] 60%|██████    | 18/30 [00:01<00:01,  9.14it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.14it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.14it/s] 70%|███████   | 21/30 [00:02<00:00,  9.14it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.14it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.14it/s] 80%|████████  | 24/30 [00:02<00:00,  9.10it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.15it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.15it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.14it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.14it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.14it/s]100%|██████████| 30/30 [00:03<00:00,  9.14it/s]100%|██████████| 30/30 [00:03<00:00,  9.20it/s]
Load LoRA latency: 6.69
End2End inference latency: 10.21
==============================
8 by william eggleston, a woman singing into a microphone, 4k, clean background
[SUYI] load_file_time=0.485
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.954
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.11
[SUYI] load_lora_into_unet_time=2.125
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.121
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.476
[SUYI] load_file_time=0.480
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.586
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.78
[SUYI] load_lora_into_unet_time=2.791
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.20it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.59it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.46it/s] 20%|██        | 6/30 [00:00<00:02,  9.37it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.30it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.26it/s] 30%|███       | 9/30 [00:00<00:02,  9.22it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.20it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.19it/s] 40%|████      | 12/30 [00:01<00:01,  9.18it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.17it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.17it/s] 50%|█████     | 15/30 [00:01<00:01,  9.16it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.16it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.15it/s] 60%|██████    | 18/30 [00:01<00:01,  9.15it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.15it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.15it/s] 70%|███████   | 21/30 [00:02<00:00,  9.15it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.15it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.15it/s] 80%|████████  | 24/30 [00:02<00:00,  9.15it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.15it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.15it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.15it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.15it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.22it/s]
Load LoRA latency: 6.76
End2End inference latency: 10.27
==============================
9 by william eggleston, a white robot passing a soccer ball to a red robot, 4k, clean background
[SUYI] load_file_time=0.494
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.879
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.04
[SUYI] load_lora_into_unet_time=2.050
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.256
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.490
[SUYI] load_file_time=0.541
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.470
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.67
[SUYI] load_lora_into_unet_time=2.677
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.25it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.57it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.44it/s] 20%|██        | 6/30 [00:00<00:02,  9.35it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.28it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.24it/s] 30%|███       | 9/30 [00:00<00:02,  9.21it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.18it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.17it/s] 40%|████      | 12/30 [00:01<00:01,  9.16it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.15it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.15it/s] 50%|█████     | 15/30 [00:01<00:01,  9.14it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.14it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.14it/s] 60%|██████    | 18/30 [00:01<00:01,  9.14it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.13it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.13it/s] 70%|███████   | 21/30 [00:02<00:00,  9.13it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.13it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.13it/s] 80%|████████  | 24/30 [00:02<00:00,  9.13it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.13it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.13it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.13it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.13it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.13it/s]100%|██████████| 30/30 [00:03<00:00,  9.13it/s]100%|██████████| 30/30 [00:03<00:00,  9.20it/s]
Load LoRA latency: 6.80
End2End inference latency: 10.32
==============================
10 by william eggleston, a plant growing on the side of a brick wall, 4k, clean background
[SUYI] load_file_time=0.485
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.968
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.12
[SUYI] load_lora_into_unet_time=2.139
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.120
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.588
[SUYI] load_file_time=0.480
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.447
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.64
[SUYI] load_lora_into_unet_time=2.654
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.29it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.58it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.45it/s] 20%|██        | 6/30 [00:00<00:02,  9.36it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.30it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.14it/s] 30%|███       | 9/30 [00:00<00:02,  9.27it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.23it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.21it/s] 40%|████      | 12/30 [00:01<00:01,  9.19it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.17it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.17it/s] 50%|█████     | 15/30 [00:01<00:01,  9.16it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.16it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.16it/s] 60%|██████    | 18/30 [00:01<00:01,  9.15it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.15it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.15it/s] 70%|███████   | 21/30 [00:02<00:00,  9.15it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.15it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.15it/s] 80%|████████  | 24/30 [00:02<00:00,  9.15it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.15it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.15it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.15it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.15it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.22it/s]
Load LoRA latency: 6.76
End2End inference latency: 10.28
==============================
11 by william eggleston, snow covering the Great Pyramid, 4k, clean background
[SUYI] load_file_time=0.486
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.982
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.14
[SUYI] load_lora_into_unet_time=2.154
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.120
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.479
[SUYI] load_file_time=0.465
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.551
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.74
[SUYI] load_lora_into_unet_time=2.754
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.26it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.57it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.44it/s] 20%|██        | 6/30 [00:00<00:02,  9.35it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.28it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.24it/s] 30%|███       | 9/30 [00:00<00:02,  9.21it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.19it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.17it/s] 40%|████      | 12/30 [00:01<00:01,  9.16it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.15it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.14it/s] 50%|█████     | 15/30 [00:01<00:01,  9.14it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.14it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.14it/s] 60%|██████    | 18/30 [00:01<00:01,  9.14it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.14it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.14it/s] 70%|███████   | 21/30 [00:02<00:00,  9.14it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.14it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.13it/s] 80%|████████  | 24/30 [00:02<00:00,  9.13it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.13it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.12it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.13it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.12it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.12it/s]100%|██████████| 30/30 [00:03<00:00,  9.12it/s]100%|██████████| 30/30 [00:03<00:00,  9.20it/s]
Load LoRA latency: 6.74
End2End inference latency: 10.26
==============================
12 by william eggleston, a photograph of a house on a mountain, 4k, clean background
[SUYI] load_file_time=0.483
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.974
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.13
[SUYI] load_lora_into_unet_time=2.143
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.121
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.475
[SUYI] load_file_time=0.465
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.450
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.64
[SUYI] load_lora_into_unet_time=2.650
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.29it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.58it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.45it/s] 20%|██        | 6/30 [00:00<00:02,  9.36it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.30it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.26it/s] 30%|███       | 9/30 [00:00<00:02,  9.22it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.20it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.19it/s] 40%|████      | 12/30 [00:01<00:01,  9.18it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.17it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.16it/s] 50%|█████     | 15/30 [00:01<00:01,  9.16it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.16it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.15it/s] 60%|██████    | 18/30 [00:01<00:01,  9.15it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.15it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.15it/s] 70%|███████   | 21/30 [00:02<00:00,  9.15it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.15it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.15it/s] 80%|████████  | 24/30 [00:02<00:00,  9.15it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.15it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.15it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.15it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.15it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.22it/s]
Load LoRA latency: 6.62
End2End inference latency: 10.13
==============================
13 by william eggleston, an owl standing on a wire, 4k, clean background
[SUYI] load_file_time=0.485
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.077
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.23
[SUYI] load_lora_into_unet_time=2.246
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.119
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.484
[SUYI] load_file_time=0.471
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.437
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.63
[SUYI] load_lora_into_unet_time=2.638
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.27it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.57it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.44it/s] 20%|██        | 6/30 [00:00<00:02,  9.35it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.29it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.24it/s] 30%|███       | 9/30 [00:00<00:02,  9.21it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.19it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.17it/s] 40%|████      | 12/30 [00:01<00:01,  9.16it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.15it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.14it/s] 50%|█████     | 15/30 [00:01<00:01,  9.14it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.13it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.13it/s] 60%|██████    | 18/30 [00:01<00:01,  9.13it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.12it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.12it/s] 70%|███████   | 21/30 [00:02<00:00,  9.12it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.12it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.12it/s] 80%|████████  | 24/30 [00:02<00:00,  9.12it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.11it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.12it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.12it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.12it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.12it/s]100%|██████████| 30/30 [00:03<00:00,  9.12it/s]100%|██████████| 30/30 [00:03<00:00,  9.19it/s]
Load LoRA latency: 6.73
End2End inference latency: 10.25
==============================
14 by william eggleston, a kids' book cover with an illustration of white dog driving a red pickup truck, 4k, clean background
[SUYI] load_file_time=0.524
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.945
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.10
[SUYI] load_lora_into_unet_time=2.112
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.122
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.584
[SUYI] load_file_time=0.476
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.460
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.65
[SUYI] load_lora_into_unet_time=2.662
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.27it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.59it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.45it/s] 20%|██        | 6/30 [00:00<00:02,  9.36it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.30it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.26it/s] 30%|███       | 9/30 [00:00<00:02,  9.22it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.20it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.19it/s] 40%|████      | 12/30 [00:01<00:01,  9.17it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.17it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.16it/s] 50%|█████     | 15/30 [00:01<00:01,  9.16it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.16it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.15it/s] 60%|██████    | 18/30 [00:01<00:01,  9.15it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.15it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.15it/s] 70%|███████   | 21/30 [00:02<00:00,  9.15it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.15it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.15it/s] 80%|████████  | 24/30 [00:02<00:00,  9.15it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.15it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.15it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.15it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.15it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.22it/s]
Load LoRA latency: 6.76
End2End inference latency: 10.27
==============================
15 by william eggleston, a subway train coming out of a tunnel, 4k, clean background
[SUYI] load_file_time=0.504
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.971
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.12
[SUYI] load_lora_into_unet_time=2.139
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.120
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.590
[SUYI] load_file_time=0.468
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.438
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.63
[SUYI] load_lora_into_unet_time=2.641
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.25it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.54it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.40it/s] 20%|██        | 6/30 [00:00<00:02,  9.32it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.26it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.22it/s] 30%|███       | 9/30 [00:00<00:02,  9.19it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.17it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.16it/s] 40%|████      | 12/30 [00:01<00:01,  9.13it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.16it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.15it/s] 50%|█████     | 15/30 [00:01<00:01,  9.14it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.14it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.14it/s] 60%|██████    | 18/30 [00:01<00:01,  9.14it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.14it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.13it/s] 70%|███████   | 21/30 [00:02<00:00,  9.14it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.14it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.13it/s] 80%|████████  | 24/30 [00:02<00:00,  9.13it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.13it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.13it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.13it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.13it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.13it/s]100%|██████████| 30/30 [00:03<00:00,  9.13it/s]100%|██████████| 30/30 [00:03<00:00,  9.19it/s]
Load LoRA latency: 6.75
End2End inference latency: 10.28
==============================
16 by william eggleston, a stone path leading away from a fountain, 4k, clean background
[SUYI] load_file_time=0.493
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.953
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.11
[SUYI] load_lora_into_unet_time=2.120
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.121
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.471
[SUYI] load_file_time=0.479
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.552
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.74
[SUYI] load_lora_into_unet_time=2.754
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.29it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.59it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.46it/s] 20%|██        | 6/30 [00:00<00:02,  9.36it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.30it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.26it/s] 30%|███       | 9/30 [00:00<00:02,  9.22it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.19it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.18it/s] 40%|████      | 12/30 [00:01<00:01,  9.17it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.16it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.15it/s] 50%|█████     | 15/30 [00:01<00:01,  9.15it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.15it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.15it/s] 60%|██████    | 18/30 [00:01<00:01,  9.15it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.15it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.15it/s] 70%|███████   | 21/30 [00:02<00:00,  9.15it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.15it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.15it/s] 80%|████████  | 24/30 [00:02<00:00,  9.15it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.15it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.15it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.15it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.15it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.21it/s]
Load LoRA latency: 6.72
End2End inference latency: 10.23
==============================
17 by william eggleston, an antique car by a beach, 4k, clean background
[SUYI] load_file_time=0.489
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.834
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.10
[SUYI] load_lora_into_unet_time=2.111
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.119
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.476
[SUYI] load_file_time=0.476
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.434
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.63
[SUYI] load_lora_into_unet_time=2.638
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.24it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.55it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.42it/s] 20%|██        | 6/30 [00:00<00:02,  9.34it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.28it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.23it/s] 30%|███       | 9/30 [00:00<00:02,  9.20it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.18it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.17it/s] 40%|████      | 12/30 [00:01<00:01,  9.16it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.15it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.15it/s] 50%|█████     | 15/30 [00:01<00:01,  9.14it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.14it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.14it/s] 60%|██████    | 18/30 [00:01<00:01,  9.14it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.14it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.14it/s] 70%|███████   | 21/30 [00:02<00:00,  9.14it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.14it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.14it/s] 80%|████████  | 24/30 [00:02<00:00,  9.14it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.14it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.14it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.14it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.14it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.14it/s]100%|██████████| 30/30 [00:03<00:00,  9.13it/s]100%|██████████| 30/30 [00:03<00:00,  9.20it/s]
Load LoRA latency: 6.59
End2End inference latency: 10.11
==============================
18 by william eggleston, black hi-top sneakers with the Nike swoosh, 4k, clean background
[SUYI] load_file_time=0.548
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.936
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.09
[SUYI] load_lora_into_unet_time=2.104
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.122
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.594
[SUYI] load_file_time=0.546
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.432
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.62
[SUYI] load_lora_into_unet_time=2.634
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.28it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.57it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.44it/s] 20%|██        | 6/30 [00:00<00:02,  9.35it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.28it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.10it/s] 30%|███       | 9/30 [00:00<00:02,  9.26it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.23it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.20it/s] 40%|████      | 12/30 [00:01<00:01,  9.19it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.17it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.17it/s] 50%|█████     | 15/30 [00:01<00:01,  9.16it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.16it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.15it/s] 60%|██████    | 18/30 [00:01<00:01,  9.15it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.15it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.15it/s] 70%|███████   | 21/30 [00:02<00:00,  9.15it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.14it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.14it/s] 80%|████████  | 24/30 [00:02<00:00,  9.14it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.15it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.15it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.15it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.15it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.21it/s]
Load LoRA latency: 6.82
End2End inference latency: 10.34
==============================
19 by william eggleston, a witch riding a broom, 4k, clean background
[SUYI] load_file_time=0.492
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.974
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.13
[SUYI] load_lora_into_unet_time=2.142
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.122
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.481
[SUYI] load_file_time=0.628
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.435
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.63
[SUYI] load_lora_into_unet_time=2.636
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.26it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.56it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.43it/s] 20%|██        | 6/30 [00:00<00:02,  9.34it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.27it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.23it/s] 30%|███       | 9/30 [00:00<00:02,  9.20it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.18it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.16it/s] 40%|████      | 12/30 [00:01<00:01,  9.15it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.14it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.13it/s] 50%|█████     | 15/30 [00:01<00:01,  9.13it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.13it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.13it/s] 60%|██████    | 18/30 [00:01<00:01,  9.13it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.13it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.13it/s] 70%|███████   | 21/30 [00:02<00:00,  9.13it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.13it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.13it/s] 80%|████████  | 24/30 [00:02<00:00,  9.13it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.13it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.13it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.13it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.13it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.13it/s]100%|██████████| 30/30 [00:03<00:00,  9.13it/s]100%|██████████| 30/30 [00:03<00:00,  9.19it/s]
Load LoRA latency: 6.78
End2End inference latency: 10.30
==============================
20 by william eggleston, a sword in a stone, 4k, clean background
[SUYI] load_file_time=0.499
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.965
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.12
[SUYI] load_lora_into_unet_time=2.133
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.122
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.481
[SUYI] load_file_time=0.481
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.454
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.77
[SUYI] load_lora_into_unet_time=2.780
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.29it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.59it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.46it/s] 20%|██        | 6/30 [00:00<00:02,  9.37it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.30it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.25it/s] 30%|███       | 9/30 [00:00<00:02,  9.22it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.20it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.19it/s] 40%|████      | 12/30 [00:01<00:01,  9.17it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.17it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.16it/s] 50%|█████     | 15/30 [00:01<00:01,  9.16it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.16it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.15it/s] 60%|██████    | 18/30 [00:01<00:01,  9.15it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.15it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.15it/s] 70%|███████   | 21/30 [00:02<00:00,  9.15it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.15it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.15it/s] 80%|████████  | 24/30 [00:02<00:00,  9.15it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.15it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.15it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.15it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.15it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.22it/s]
Load LoRA latency: 6.78
End2End inference latency: 10.29
==============================
21 by william eggleston, cash on a wooden table, 4k, clean background
[SUYI] load_file_time=0.546
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.855
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.01
[SUYI] load_lora_into_unet_time=2.026
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.124
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.592
[SUYI] load_file_time=0.491
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.414
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.61
[SUYI] load_lora_into_unet_time=2.618
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.24it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.56it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.43it/s] 20%|██        | 6/30 [00:00<00:02,  9.34it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.28it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.23it/s] 30%|███       | 9/30 [00:00<00:02,  9.16it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.20it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.18it/s] 40%|████      | 12/30 [00:01<00:01,  9.17it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.16it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.15it/s] 50%|█████     | 15/30 [00:01<00:01,  9.15it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.14it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.14it/s] 60%|██████    | 18/30 [00:01<00:01,  9.14it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.13it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.13it/s] 70%|███████   | 21/30 [00:02<00:00,  9.13it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.13it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.13it/s] 80%|████████  | 24/30 [00:02<00:00,  9.13it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.13it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.13it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.13it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.13it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.13it/s]100%|██████████| 30/30 [00:03<00:00,  9.13it/s]100%|██████████| 30/30 [00:03<00:00,  9.20it/s]
Load LoRA latency: 6.68
End2End inference latency: 10.20
==============================
22 by william eggleston, the Millennium Wheel in a snow storm, 4k, clean background
[SUYI] load_file_time=0.503
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.972
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.13
[SUYI] load_lora_into_unet_time=2.141
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.118
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.583
[SUYI] load_file_time=0.483
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.407
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.60
[SUYI] load_lora_into_unet_time=2.611
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.27it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.57it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.44it/s] 20%|██        | 6/30 [00:00<00:02,  9.35it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.29it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.25it/s] 30%|███       | 9/30 [00:00<00:02,  9.22it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.20it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.18it/s] 40%|████      | 12/30 [00:01<00:01,  9.17it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.16it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.16it/s] 50%|█████     | 15/30 [00:01<00:01,  9.16it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.16it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.15it/s] 60%|██████    | 18/30 [00:01<00:01,  9.15it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.15it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.15it/s] 70%|███████   | 21/30 [00:02<00:00,  9.15it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.15it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.14it/s] 80%|████████  | 24/30 [00:02<00:00,  9.14it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.15it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.15it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.15it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.15it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.21it/s]
Load LoRA latency: 6.72
End2End inference latency: 10.23
==============================
23 by william eggleston, a knight holding a long sword, 4k, clean background
[SUYI] load_file_time=0.490
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.927
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.08
[SUYI] load_lora_into_unet_time=2.097
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.119
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.474
[SUYI] load_file_time=0.479
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.542
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.73
[SUYI] load_lora_into_unet_time=2.744
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.24it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.55it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.42it/s] 20%|██        | 6/30 [00:00<00:02,  9.33it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.27it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.23it/s] 30%|███       | 9/30 [00:00<00:02,  9.20it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.18it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.17it/s] 40%|████      | 12/30 [00:01<00:01,  9.16it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.15it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.15it/s] 50%|█████     | 15/30 [00:01<00:01,  9.14it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.14it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.14it/s] 60%|██████    | 18/30 [00:01<00:01,  9.14it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.13it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.14it/s] 70%|███████   | 21/30 [00:02<00:00,  9.13it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.13it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.13it/s] 80%|████████  | 24/30 [00:02<00:00,  9.13it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.13it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.13it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.13it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.13it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.13it/s]100%|██████████| 30/30 [00:03<00:00,  9.13it/s]100%|██████████| 30/30 [00:03<00:00,  9.20it/s]
Load LoRA latency: 6.69
End2End inference latency: 10.20
==============================
24 by william eggleston, a smiling sloth, 4k, clean background
[SUYI] load_file_time=0.497
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.848
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.00
[SUYI] load_lora_into_unet_time=2.018
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.227
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.485
[SUYI] load_file_time=0.481
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.463
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.66
[SUYI] load_lora_into_unet_time=2.666
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.28it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.58it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.45it/s] 20%|██        | 6/30 [00:00<00:02,  9.36it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.29it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.25it/s] 30%|███       | 9/30 [00:00<00:02,  9.22it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.20it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.18it/s] 40%|████      | 12/30 [00:01<00:01,  9.18it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.17it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.16it/s] 50%|█████     | 15/30 [00:01<00:01,  9.16it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.15it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.15it/s] 60%|██████    | 18/30 [00:01<00:01,  9.15it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.15it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.15it/s] 70%|███████   | 21/30 [00:02<00:00,  9.15it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.15it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.15it/s] 80%|████████  | 24/30 [00:02<00:00,  9.15it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.15it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.15it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.14it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.15it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.14it/s]100%|██████████| 30/30 [00:03<00:00,  9.14it/s]100%|██████████| 30/30 [00:03<00:00,  9.21it/s]
Load LoRA latency: 6.65
End2End inference latency: 10.17
==============================
25 by william eggleston, a bundle of blue and yellow flowers in a vase, 4k, clean background
[SUYI] load_file_time=0.536
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.952
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.11
[SUYI] load_lora_into_unet_time=2.121
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.123
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.593
[SUYI] load_file_time=0.467
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.426
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.62
[SUYI] load_lora_into_unet_time=2.625
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.26it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.56it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.44it/s] 20%|██        | 6/30 [00:00<00:02,  9.34it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.28it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.24it/s] 30%|███       | 9/30 [00:00<00:02,  9.21it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.19it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.17it/s] 40%|████      | 12/30 [00:01<00:01,  9.16it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.15it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.15it/s] 50%|█████     | 15/30 [00:01<00:01,  9.14it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.14it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.14it/s] 60%|██████    | 18/30 [00:01<00:01,  9.14it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.13it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.14it/s] 70%|███████   | 21/30 [00:02<00:00,  9.14it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.14it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.14it/s] 80%|████████  | 24/30 [00:02<00:00,  9.13it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.13it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.13it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.13it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.14it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.13it/s]100%|██████████| 30/30 [00:03<00:00,  9.14it/s]100%|██████████| 30/30 [00:03<00:00,  9.20it/s]
Load LoRA latency: 6.75
End2End inference latency: 10.27
==============================
26 by william eggleston, a cat coming through a cat door, 4k, clean background
[SUYI] load_file_time=0.482
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.971
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.13
[SUYI] load_lora_into_unet_time=2.139
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.120
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.473
[SUYI] load_file_time=0.560
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.587
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.78
[SUYI] load_lora_into_unet_time=2.789
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.28it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.58it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.45it/s] 20%|██        | 6/30 [00:00<00:02,  9.36it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.30it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.25it/s] 30%|███       | 9/30 [00:00<00:02,  9.22it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.20it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.18it/s] 40%|████      | 12/30 [00:01<00:01,  9.17it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.16it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.05it/s] 50%|█████     | 15/30 [00:01<00:01,  9.20it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.19it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.17it/s] 60%|██████    | 18/30 [00:01<00:01,  9.17it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.16it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.16it/s] 70%|███████   | 21/30 [00:02<00:00,  9.16it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.15it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.15it/s] 80%|████████  | 24/30 [00:02<00:00,  9.15it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.15it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.15it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.15it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.15it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.22it/s]
Load LoRA latency: 6.84
End2End inference latency: 10.36
==============================
27 by william eggleston, a pickup truck going up a mountain switchback, 4k, clean background
[SUYI] load_file_time=0.486
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.953
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.11
[SUYI] load_lora_into_unet_time=2.123
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.123
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.475
[SUYI] load_file_time=0.471
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.445
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.64
[SUYI] load_lora_into_unet_time=2.648
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.26it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.57it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.44it/s] 20%|██        | 6/30 [00:00<00:02,  9.34it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.28it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.24it/s] 30%|███       | 9/30 [00:00<00:02,  9.21it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.19it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.17it/s] 40%|████      | 12/30 [00:01<00:01,  9.16it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.14it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.14it/s] 50%|█████     | 15/30 [00:01<00:01,  9.14it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.15it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.12it/s] 60%|██████    | 18/30 [00:01<00:01,  9.15it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.14it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.14it/s] 70%|███████   | 21/30 [00:02<00:00,  9.13it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.14it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.14it/s] 80%|████████  | 24/30 [00:02<00:00,  9.13it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.14it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.14it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.14it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.13it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.14it/s]100%|██████████| 30/30 [00:03<00:00,  9.13it/s]100%|██████████| 30/30 [00:03<00:00,  9.20it/s]
Load LoRA latency: 6.61
End2End inference latency: 10.13
==============================
28 by william eggleston, a street in Paris, 4k, clean background
[SUYI] load_file_time=0.488
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.983
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.14
[SUYI] load_lora_into_unet_time=2.152
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.122
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.594
[SUYI] load_file_time=0.475
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.467
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.66
[SUYI] load_lora_into_unet_time=2.671
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.29it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.58it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.45it/s] 20%|██        | 6/30 [00:00<00:02,  9.36it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.30it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.25it/s] 30%|███       | 9/30 [00:00<00:02,  9.22it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.20it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.19it/s] 40%|████      | 12/30 [00:01<00:01,  9.17it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.17it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.16it/s] 50%|█████     | 15/30 [00:01<00:01,  9.16it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.16it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.16it/s] 60%|██████    | 18/30 [00:01<00:01,  9.15it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.15it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.15it/s] 70%|███████   | 21/30 [00:02<00:00,  9.15it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.15it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.15it/s] 80%|████████  | 24/30 [00:02<00:00,  9.15it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.15it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.15it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.15it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.15it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.15it/s]100%|██████████| 30/30 [00:03<00:00,  9.22it/s]
Load LoRA latency: 6.79
End2End inference latency: 10.30
==============================
29 by william eggleston, a teddy bear on a skateboard, 4k, clean background
[SUYI] load_file_time=0.476
Before inject 1680
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 1.976
After inject 3124
[SUYI] inject_adapter_in_model_with_weights time: 2.13
[SUYI] load_lora_into_unet_time=2.145
[SUYI] len(text_encoder_state_dict) 144
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.120
[SUYI] len(text_encoder_2_state_dict) 384
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 0.583
[SUYI] load_file_time=0.483
Before inject 3124
LoraModel not implement _pre_injection_hook() yet
[SUYI] inject_adapter time 2.482
After inject 4568
[SUYI] inject_adapter_in_model_with_weights time: 2.67
[SUYI] load_lora_into_unet_time=2.683
[SUYI] len(text_encoder_state_dict) 0
[SUYI] len(text_encoder_2_state_dict) 0
lora_scale: 1.0, safe_fusing: False, adapter_names: None
  0%|          | 0/30 [00:00<?, ?it/s]  7%|▋         | 2/30 [00:00<00:02, 10.24it/s] 13%|█▎        | 4/30 [00:00<00:02,  9.56it/s] 17%|█▋        | 5/30 [00:00<00:02,  9.43it/s] 20%|██        | 6/30 [00:00<00:02,  9.34it/s] 23%|██▎       | 7/30 [00:00<00:02,  9.28it/s] 27%|██▋       | 8/30 [00:00<00:02,  9.24it/s] 30%|███       | 9/30 [00:00<00:02,  9.20it/s] 33%|███▎      | 10/30 [00:01<00:02,  9.18it/s] 37%|███▋      | 11/30 [00:01<00:02,  9.17it/s] 40%|████      | 12/30 [00:01<00:01,  9.16it/s] 43%|████▎     | 13/30 [00:01<00:01,  9.15it/s] 47%|████▋     | 14/30 [00:01<00:01,  9.15it/s] 50%|█████     | 15/30 [00:01<00:01,  9.14it/s] 53%|█████▎    | 16/30 [00:01<00:01,  9.14it/s] 57%|█████▋    | 17/30 [00:01<00:01,  9.14it/s] 60%|██████    | 18/30 [00:01<00:01,  9.13it/s] 63%|██████▎   | 19/30 [00:02<00:01,  9.13it/s] 67%|██████▋   | 20/30 [00:02<00:01,  9.13it/s] 70%|███████   | 21/30 [00:02<00:00,  9.13it/s] 73%|███████▎  | 22/30 [00:02<00:00,  9.13it/s] 77%|███████▋  | 23/30 [00:02<00:00,  9.13it/s] 80%|████████  | 24/30 [00:02<00:00,  9.13it/s] 83%|████████▎ | 25/30 [00:02<00:00,  9.13it/s] 87%|████████▋ | 26/30 [00:02<00:00,  9.13it/s] 90%|█████████ | 27/30 [00:02<00:00,  9.13it/s] 93%|█████████▎| 28/30 [00:03<00:00,  9.13it/s] 97%|█████████▋| 29/30 [00:03<00:00,  9.13it/s]100%|██████████| 30/30 [00:03<00:00,  9.13it/s]100%|██████████| 30/30 [00:03<00:00,  9.20it/s]
Load LoRA latency: 6.77
End2End inference latency: 10.29
==============================
